# ======================================================================================
# LangGraph ê¸°ë°˜ ê°œì¸í™” í˜œíƒ ì¶”ì²œ RAG ì‹œìŠ¤í…œ - ë©”ì¸ í´ë˜ìŠ¤
# ======================================================================================
import json
import chromadb
import os
import sys
import re
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Set
import math
from collections import defaultdict

# LangGraph imports
from langgraph.graph import StateGraph, END
#from langgraph.checkpoint.sqlite import SqliteSaver

# ë¡œì»¬ imports
from tools.api_utils import PerplexityAPI, EmbeddingExecutor, PersonalizedScoreCalculator
from tools.rag_types import RAGState

# ê¸°ì¡´ ëª¨ë“ˆë“¤ (ë™ì¼í•œ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •)
from tools.multi_category_parser import MultiCategoryQueryParser
from tools.multi_category_dummy_data import MULTI_CATEGORY_DATA
from tools.user_history_data import create_sample_user_history


class LangGraphRAGSystem:
    """LangGraph ê¸°ë°˜ RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_path="./cafe_vector_db", collection_name="cafe_events"):
        self.db_path = db_path
        self.collection_name = collection_name
        self.client = None
        self.collection = None
        self.score_calculator = PersonalizedScoreCalculator()
        self.perplexity_api = PerplexityAPI()
        
        # ì„ë² ë”© API ì„¤ì •
        api_key = 'Bearer nv-53f7a8c4abe74e20ab90446ed46ba79fvozJ'
        self.embedding_executor = EmbeddingExecutor(
            host='clovastudio.stream.ntruss.com',
            api_key=api_key,
            request_id='93ae6593a47d4437b634f2cbc5282896'
        )
        
        # DB ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ ìºì‹œ
        self.available_brands: Set[str] = set()
        self.available_categories: Set[str] = set()
        self.db_metadata_loaded = False
        
        # LangGraph ì´ˆê¸°í™”
        self.graph = self._build_graph()
        
    def connect_database(self) -> bool:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°"""
        try:
            if not os.path.exists(self.db_path):
                print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
                return False
            
            self.client = chromadb.PersistentClient(path=self.db_path)
            collections = self.client.list_collections()
            
            if collections:
                self.collection = collections[0]
                self.collection_name = collections[0].name
                self._load_database_metadata()
                print(f"âœ… RAG DB ì—°ê²° ì„±ê³µ (ì´ {self.collection.count()}ê°œ ë¬¸ì„œ)")
                return True
            else:
                print("âŒ ì»¬ë ‰ì…˜ì´ ì—†ìŠµë‹ˆë‹¤")
                return False
                
        except Exception as e:
            print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

    def _load_database_metadata(self) -> None:
        """DB ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            all_results = self.collection.get(include=["metadatas"])
            
            if all_results and all_results.get('metadatas'):
                for metadata in all_results['metadatas']:
                    if metadata:
                        brand = metadata.get('brand')
                        category = metadata.get('category')
                        
                        if brand:
                            self.available_brands.add(brand.strip())
                        if category:
                            self.available_categories.add(category.strip())
                
                self.db_metadata_loaded = True
                print(f"âœ… DB ë©”íƒ€ë°ì´í„° ë¡œë“œ ì™„ë£Œ: ë¸Œëœë“œ {len(self.available_brands)}ê°œ, ì¹´í…Œê³ ë¦¬ {len(self.available_categories)}ê°œ")
                
        except Exception as e:
            print(f"âŒ DB ë©”íƒ€ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
            self.db_metadata_loaded = False

    # ======================================================================================
    # LangGraph ë…¸ë“œë“¤
    # ======================================================================================
    
    def analyze_query_node(self, state: RAGState) -> RAGState:
        """ì¿¼ë¦¬ ë¶„ì„ ë…¸ë“œ"""
        query = state["query"]
        debug = state.get("debug", False)
        
        if debug:
            print(f"ğŸ” ì¿¼ë¦¬ ë¶„ì„ ì‹œì‘: {query}")
        
        # ì¿¼ë¦¬ ë¶„ì„
        analysis = MultiCategoryQueryParser.analyze_query_intent(query)
        filters, parsed_info = MultiCategoryQueryParser.build_multi_category_filters(query, debug)
        
        # ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
        extracted_brands = self._extract_brands_from_query(query, debug)
        extracted_categories = self._extract_categories_from_query(query, debug)
        
        # ê°œì¸í™” ìš”ì²­ íŒë‹¨
        is_personalization = self._is_personalization_query(query)
        
        state.update({
            "query_analysis": analysis,
            "parsed_info": parsed_info,
            "filters": filters,
            "extracted_brands": extracted_brands,
            "extracted_categories": extracted_categories,
            "is_personalization": is_personalization,
            "next_action": "create_user_profile"
        })
        
        if debug:
            print(f"âœ… ì¿¼ë¦¬ ë¶„ì„ ì™„ë£Œ: ë¸Œëœë“œ={extracted_brands}, ì¹´í…Œê³ ë¦¬={extracted_categories}, ê°œì¸í™”={is_personalization}")
        
        return state

    def create_user_profile_node(self, state: RAGState) -> RAGState:
        """ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ë…¸ë“œ"""
        user_history = state["user_history"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ‘¤ ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì¤‘...")
        
        user_profile = self.create_user_profile(user_history)
        
        state.update({
            "user_profile_data": user_profile,
            "next_action": "validate_query"
        })
        
        if debug:
            print(f"âœ… ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„± ì™„ë£Œ: ì´ ì†Œë¹„ {user_profile['total_spending']:,.0f}ì›")
        
        return state

    def validate_query_node(self, state: RAGState) -> RAGState:
        """ì¿¼ë¦¬ ê²€ì¦ ë…¸ë“œ"""
        query = state["query"]
        analysis = state["query_analysis"]
        parsed_info = state["parsed_info"]
        extracted_brands = state["extracted_brands"]
        is_personalization = state["is_personalization"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ”§ ì¿¼ë¦¬ ê²€ì¦ ì‹œì‘...")
        
        validation_result = self._validate_query_improved(
            query, analysis, parsed_info, extracted_brands, is_personalization, debug
        )
        
        state.update({
            "validation_result": validation_result
        })
        
        if validation_result['valid']:
            state["next_action"] = "perform_direct_search"
            if debug:
                print("âœ… ì¿¼ë¦¬ ê²€ì¦ í†µê³¼ - ê²€ìƒ‰ ì§„í–‰")
        else:
            state["next_action"] = "perform_perplexity_search"
            if debug:
                print("âŒ ì¿¼ë¦¬ ê²€ì¦ ì‹¤íŒ¨ - Perplexity ê²€ìƒ‰ìœ¼ë¡œ ì „í™˜")
        
        return state

    def perform_direct_search_node(self, state: RAGState) -> RAGState:
        """ì§ì ‘ ê²€ìƒ‰ ë…¸ë“œ (ë¸Œëœë“œ/ì¹´í…Œê³ ë¦¬)"""
        query = state["query"]
        extracted_brands = state["extracted_brands"]
        extracted_categories = state["extracted_categories"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ¯ ì§ì ‘ ê²€ìƒ‰ ì‹œë„...")
        
        results = []
        
        # ë¸Œëœë“œ ìš°ì„  ê²€ìƒ‰
        if extracted_brands:
            results = self._try_direct_brand_search(query, 10, debug)
            if results:
                if debug:
                    print(f"âœ… ë¸Œëœë“œ ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ")
                state.update({
                    "direct_search_results": results,
                    "next_action": "apply_personalization"
                })
                return state
        
        # ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
        if extracted_categories:
            results = self._try_direct_category_search(query, extracted_categories, 10, debug)
            if results:
                if debug:
                    print(f"âœ… ì¹´í…Œê³ ë¦¬ ì§ì ‘ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ")
                state.update({
                    "direct_search_results": results,
                    "next_action": "apply_personalization"
                })
                return state
        
        # ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨
        state.update({
            "direct_search_results": [],
            "next_action": "perform_vector_search"
        })
        
        if debug:
            print("âš ï¸ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨ - ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰")
        
        return state

    def perform_vector_search_node(self, state: RAGState) -> RAGState:
        """ë²¡í„° ê²€ìƒ‰ ë…¸ë“œ"""
        query = state["query"]
        filters = state["filters"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘...")
        
        try:
            # ì„ë² ë”© ìƒì„±
            request_data = {"text": query}
            query_vector = self.embedding_executor.execute(request_data)
            
            if not query_vector:
                raise Exception("ì„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # ë²¡í„° ì •ê·œí™”
            query_vector_array = np.array(query_vector)
            norm = np.linalg.norm(query_vector_array)
            if norm > 0:
                normalized_query_vector = (query_vector_array / norm).tolist()
            else:
                normalized_query_vector = query_vector
            
            # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_embeddings=[normalized_query_vector],
                n_results=20,
                include=["metadatas", "distances", "documents"]
            )
            
            if results and results.get('ids') and results['ids'][0]:
                # ê²°ê³¼ í¬ë§·íŒ…
                formatted_results = []
                ids = results['ids'][0]
                metadatas = results['metadatas'][0] 
                distances = results['distances'][0]
                documents = results['documents'][0]
                
                for i in range(len(ids)):
                    formatted_results.append({
                        "id": ids[i],
                        "metadata": metadatas[i],
                        "distance": distances[i],
                        "document": documents[i],
                        "vector_rank": i + 1
                    })
                
                state.update({
                    "vector_search_results": formatted_results,
                    "next_action": "apply_personalization"
                })
                
                if debug:
                    print(f"âœ… ë²¡í„° ê²€ìƒ‰ ì„±ê³µ: {len(formatted_results)}ê°œ")
                
                return state
            else:
                raise Exception("ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                
        except Exception as e:
            if debug:
                print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            
            state.update({
                "vector_search_results": [],
                "next_action": "perform_text_search"
            })
            
            return state

    def perform_text_search_node(self, state: RAGState) -> RAGState:
        """í…ìŠ¤íŠ¸ í´ë°± ê²€ìƒ‰ ë…¸ë“œ"""
        query = state["query"]
        filters = state["filters"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ”„ í…ìŠ¤íŠ¸ í´ë°± ê²€ìƒ‰ ì‹œì‘...")
        
        try:
            results = self._fallback_text_search(query, filters, 10, debug)
            
            if results:
                state.update({
                    "text_search_results": results,
                    "next_action": "apply_personalization"
                })
                
                if debug:
                    print(f"âœ… í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ")
            else:
                state.update({
                    "text_search_results": [],
                    "next_action": "perform_perplexity_search"
                })
                
                if debug:
                    print("âŒ í…ìŠ¤íŠ¸ ê²€ìƒ‰ë„ ì‹¤íŒ¨ - Perplexityë¡œ ì „í™˜")
                    
        except Exception as e:
            if debug:
                print(f"âŒ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            
            state.update({
                "text_search_results": [],
                "next_action": "perform_perplexity_search"
            })
        
        return state
    def perform_perplexity_search_node(self, state: RAGState) -> RAGState:
        """Perplexity ì‹¤ì‹œê°„ ê²€ìƒ‰ ë…¸ë“œ - JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
        query = state["query"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ”— Perplexity ì‹¤ì‹œê°„ JSON ê²€ìƒ‰ ì‹œì‘...")
        
        # Perplexityê°€ ì§ì ‘ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ ìˆ˜ì •
        search_query = f"""ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ê´€ë ¨ëœ ë¸Œëœë“œë¥¼ ì°¾ì•„, 2025ë…„ 8ì›” í˜„ì¬ ì§„í–‰ì¤‘ì¸ ìµœì‹  í˜œíƒ ì •ë³´ë¥¼ ì°¾ì•„ì¤˜.
        
        ì‚¬ìš©ì ì§ˆë¬¸: "{query}"

        - ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì•¼ í•´.
        - ë‹¤ë¥¸ ì„¤ëª…, ì¸ì‚¬, ì½”ë“œ ë¸”ë¡(```json) ì—†ì´ ìˆœìˆ˜í•œ JSON ë‚´ìš©ë§Œ ì¶œë ¥í•´ì¤˜.
        - ë§Œì•½ ì—¬ëŸ¬ ë¸Œëœë“œ í˜œíƒì„ ì°¾ìœ¼ë©´, ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í˜œíƒ í•˜ë‚˜ë§Œ JSON ê°ì²´ë¡œ ë§Œë“¤ì–´ì¤˜.
        - ì •ë³´ê°€ ì—†ê±°ë‚˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ ì—†ëŠ” ë‚´ìš©ì´ë©´ ë¹ˆ JSON ê°ì²´ {{}}ë¥¼ ë°˜í™˜í•´.
        - ë‚ ì§œê°€ ì§€ë‚œ ì´ë²¤íŠ¸ëŠ” ì œì™¸í•˜ê³  í˜„ì¬ ìœ íš¨í•œ ì •ë³´ë§Œ í¬í•¨í•´ì¤˜.

        JSON í˜•ì‹:
        {{
        "brand": "[ë¸Œëœë“œëª…]",
        "eventName": "[ì´ë²¤íŠ¸ë‚˜ í”„ë¡œëª¨ì…˜ ì´ë¦„]",
        "benefit": "[í• ì¸ìœ¨, ì¿ í° ë“± í˜œíƒ í•µì‹¬ ë‚´ìš©]",
        "period": "[ì‹œì‘ì¼ ~ ì¢…ë£Œì¼, ì˜ˆ: 2025-08-01 ~ 2025-08-31]"
        }}
        """
        
        # ìµœì¢…ì ìœ¼ë¡œ stateì— ì €ì¥ë  ë‚´ìš©
        final_content = {}

        try:
            result = self.perplexity_api.search(search_query)
            
            if result.get('success'):
                raw_content = result.get('content', '{}')
                
                # Perplexityê°€ ë°˜í™˜í•œ ë¬¸ìì—´ì„ JSON ê°ì²´ë¡œ íŒŒì‹±
                # LLMì´ ì™„ë²½í•œ JSONì„ ìƒì„±í•˜ì§€ ëª»í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì˜ˆì™¸ ì²˜ë¦¬ í•„ìˆ˜
                try:
                    parsed_json = json.loads(raw_content)
                    # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ë˜ë©´ í•´ë‹¹ ë‚´ìš©ì„ final_contentë¡œ ì‚¬ìš©
                    final_content = parsed_json
                except json.JSONDecodeError:
                    # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ, ì›ë³¸ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì—ëŸ¬ ë©”ì‹œì§€ ìƒì„±
                    final_content = {
                        "error": "Perplexityê°€ ë°˜í™˜í•œ ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                        "raw_response": raw_content
                    }
            else:
                # API í˜¸ì¶œ ìì²´ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
                final_content = {
                    "error": "Perplexity API ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.",
                    "details": result.get('error', 'Unknown error')
                }
        except Exception as e:
            # ê·¸ ì™¸ ì˜ˆì™¸ ì²˜ë¦¬
            final_content = {
                "error": "Perplexity ê²€ìƒ‰ ë…¸ë“œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "details": str(e)
            }

        # ìµœì¢… ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ì€ í˜•íƒœì˜ JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        response_str = json.dumps(final_content, ensure_ascii=False, indent=4)
        
        state.update({
            "perplexity_results": response_str, # ì¼ê´€ì„±ì„ ìœ„í•´ ë¬¸ìì—´ ì €ì¥
            "response": response_str,
            "next_action": "end"
        })
        
        if debug:
            print("âœ… Perplexity ê²€ìƒ‰ ì™„ë£Œ")
            
        return state
        

    def apply_personalization_node(self, state: RAGState) -> RAGState:
        """ê°œì¸í™” ìŠ¤ì½”ì–´ë§ ë…¸ë“œ"""
        user_profile = state["user_profile_data"]
        parsed_info = state["parsed_info"]
        debug = state.get("debug", False)
        
        # ê²€ìƒ‰ ê²°ê³¼ í†µí•©
        all_results = []
        all_results.extend(state.get("direct_search_results", []))
        all_results.extend(state.get("vector_search_results", []))
        all_results.extend(state.get("text_search_results", []))
        
        if debug:
            print(f"ğŸ”„ ê°œì¸í™” ìŠ¤ì½”ì–´ë§: {len(all_results)}ê°œ ê²°ê³¼")
        
        if not all_results:
            state.update({
                "personalized_results": [],
                "next_action": "perform_perplexity_search"
            })
            return state
        
        # ê°œì¸í™” ìŠ¤ì½”ì–´ë§ ì ìš©
        scored_results = self._apply_personalization_scoring_readonly(
            all_results, user_profile, parsed_info, debug
        )
        
        state.update({
            "personalized_results": scored_results,
            "next_action": "generate_final_results"
        })
        
        if debug:
            print(f"âœ… ê°œì¸í™” ìŠ¤ì½”ì–´ë§ ì™„ë£Œ: {len(scored_results)}ê°œ")
        
        return state

    def generate_final_results_node(self, state: RAGState) -> RAGState:
        """ê²°ê³¼ ìƒì„± ë…¸ë“œ"""
        personalized_results = state["personalized_results"]
        user_profile = state["user_profile_data"]
        parsed_info = state["parsed_info"]
        debug = state.get("debug", False)
        
        if debug:
            print("ğŸ“‹ ìµœì¢… ê²°ê³¼ ìƒì„± ì¤‘...")
        
        # ìµœì¢… ìˆœìœ„ ê²°ì • ë° ê²°ê³¼ ì„ íƒ
        final_results = self._rank_and_select_results(
            personalized_results, user_profile, 5, debug
        )
        
        # ê²°ê³¼ ì¶œë ¥ ìƒì„±
        if final_results:
            response = self._generate_results_readonly(final_results, user_profile, parsed_info)
        else:
            response = "âŒ í•´ë‹¹ ì¡°ê±´ì— ë§ëŠ” í˜œíƒ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
            
        state.update({
            "final_results": final_results,
            "response": response,
            "next_action": "end"
        })
        
        if debug:
            print("âœ… ìµœì¢… ê²°ê³¼ ìƒì„± ì™„ë£Œ")
        
        return state

    # ======================================================================================
    # ë¼ìš°íŒ… í•¨ìˆ˜
    # ======================================================================================
    
    def route_next_action(self, state: RAGState) -> str:
        """ë‹¤ìŒ ì•¡ì…˜ìœ¼ë¡œ ë¼ìš°íŒ…"""
        next_action = state.get("next_action", "end")
        
        if next_action == "end":
            return END
        
        return next_action

    # ======================================================================================
    # ê·¸ë˜í”„ êµ¬ì„±
    # ======================================================================================
    
    def _build_graph(self) -> StateGraph:
        """LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±"""
        workflow = StateGraph(RAGState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("analyze_query", self.analyze_query_node)
        workflow.add_node("create_user_profile", self.create_user_profile_node)
        workflow.add_node("validate_query", self.validate_query_node)
        workflow.add_node("perform_direct_search", self.perform_direct_search_node)
        workflow.add_node("perform_vector_search", self.perform_vector_search_node)
        workflow.add_node("perform_text_search", self.perform_text_search_node)
        workflow.add_node("perform_perplexity_search", self.perform_perplexity_search_node)
        workflow.add_node("apply_personalization", self.apply_personalization_node)
        workflow.add_node("generate_final_results", self.generate_final_results_node)
        
        # ì—£ì§€ ì¶”ê°€
        workflow.set_entry_point("analyze_query")
        
        workflow.add_conditional_edges(
            "analyze_query",
            self.route_next_action,
            {
                "create_user_profile": "create_user_profile",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "create_user_profile",
            self.route_next_action,
            {
                "validate_query": "validate_query",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "validate_query",
            self.route_next_action,
            {
                "perform_direct_search": "perform_direct_search",
                "perform_perplexity_search": "perform_perplexity_search",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "perform_direct_search",
            self.route_next_action,
            {
                "perform_vector_search": "perform_vector_search",
                "apply_personalization": "apply_personalization",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "perform_vector_search",
            self.route_next_action,
            {
                "perform_text_search": "perform_text_search",
                "apply_personalization": "apply_personalization",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "perform_text_search",
            self.route_next_action,
            {
                "perform_perplexity_search": "perform_perplexity_search",
                "apply_personalization": "apply_personalization",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "perform_perplexity_search",
            self.route_next_action,
            {
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "apply_personalization",
            self.route_next_action,
            {
                "generate_final_results": "generate_final_results",
                "perform_perplexity_search": "perform_perplexity_search",
                END: END
            }
        )
        
        workflow.add_conditional_edges(
            "generate_final_results",
            self.route_next_action,
            {
                END: END
            }
        )
        
        return workflow.compile()

    # ======================================================================================
    # ê¸°ì¡´ ì‹œìŠ¤í…œì˜ í—¬í¼ ë©”ì„œë“œë“¤
    # ======================================================================================
    
    def create_user_profile(self, user_spending_history: List[Dict]) -> Dict:
        """ì‚¬ìš©ì í”„ë¡œí•„ ìƒì„±"""
        profile = {
            'brand_counts': defaultdict(int),
            'category_counts': defaultdict(int),
            'brand_spending': defaultdict(float),
            'category_spending': defaultdict(float),
            'total_transactions': 0,
            'total_spending': 0.0,
            'recent_brands': [],
            'preferred_categories': [],
            'avg_spending_per_brand': {},
            'spending_timeline': []
        }
        
        current_date = datetime.now()
        recent_threshold = current_date - timedelta(days=7)
        
        for transaction in user_spending_history:
            brand = transaction['brand']
            category = transaction.get('category', 'Unknown')
            amount = transaction['amount']
            date = datetime.fromisoformat(transaction['date']) if isinstance(transaction['date'], str) else transaction['date']
            
            # ê¸°ë³¸ í†µê³„
            profile['brand_counts'][brand] += 1
            profile['category_counts'][category] += 1
            profile['brand_spending'][brand] += amount
            profile['category_spending'][category] += amount
            profile['total_transactions'] += 1
            profile['total_spending'] += amount
            
            # ìµœê·¼ ë¸Œëœë“œ
            if date >= recent_threshold:
                recency_weight = self._calculate_recency_weight(date, current_date)
                profile['recent_brands'].append({
                    'brand': brand,
                    'category': category,
                    'amount': amount,
                    'weight': recency_weight,
                    'date': date
                })
            
            # ì‹œê°„ìˆœ ê¸°ë¡
            profile['spending_timeline'].append({
                'brand': brand,
                'category': category, 
                'amount': amount,
                'date': date
            })
        
        # ë¸Œëœë“œë³„ í‰ê·  ì†Œë¹„ì•¡ ê³„ì‚°
        for brand, total_amount in profile['brand_spending'].items():
            count = profile['brand_counts'][brand]
            profile['avg_spending_per_brand'][brand] = total_amount / count
        
        # ì„ í˜¸ ì¹´í…Œê³ ë¦¬ ì •ë ¬
        profile['preferred_categories'] = sorted(
            profile['category_counts'].items(), 
            key=lambda x: x[1], 
            reverse=True
        )
        
        return dict(profile)

    def _calculate_recency_weight(self, spending_date: datetime, current_date: datetime) -> float:
        """ìµœê·¼ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        days_diff = (current_date - spending_date).days
        if days_diff <= 7:
            return 1.0
        elif days_diff <= 30:
            return 0.8
        elif days_diff <= 90:
            return 0.5
        else:
            return 0.2

    def _extract_brands_from_query(self, query: str, debug: bool = False) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ë¸Œëœë“œ ì¶”ì¶œ"""
        found_brands = []
        
        known_brand_patterns = {
            'ìŠ¤íƒ€ë²…ìŠ¤': [r'ìŠ¤íƒ€ë²…ìŠ¤', r'starbucks'],
            'ì´ë””ì•¼': [r'ì´ë””ì•¼', r'ediya'],
            'íˆ¬ì¸í”Œë ˆì´ìŠ¤': [r'íˆ¬ì¸', r'íˆ¬ì¸í”Œë ˆì´ìŠ¤', r'twosome'],
            'ë§¥ë„ë‚ ë“œ': [r'ë§¥ë„ë‚ ë“œ', r'ë§¥ë‚ ', r'mcdonald'],
            'ë²„ê±°í‚¹': [r'ë²„ê±°í‚¹', r'burgerking'],
            'KFC': [r'kfc', r'ì¼€ì´ì—í”„ì”¨'],
            'ì´ë§ˆíŠ¸': [r'ì´ë§ˆíŠ¸', r'emart'],
            'í™ˆí”ŒëŸ¬ìŠ¤': [r'í™ˆí”ŒëŸ¬ìŠ¤', r'homeplus'],
            'ë¡¯ë°ë§ˆíŠ¸': [r'ë¡¯ë°ë§ˆíŠ¸', r'lotte'],
            'ì¿ íŒ¡': [r'ì¿ íŒ¡', r'coupang'],
            'ì§€ë§ˆì¼“': [r'ì§€ë§ˆì¼“', r'gmarket'],
            '11ë²ˆê°€': [r'11ë²ˆê°€', r'ì‹­ì¼ë²ˆê°€'],
            'GS25': [r'gs25', r'ì§€ì—ìŠ¤'],
            'CU': [r'cu', r'ì”¨ìœ '],
            'ì„¸ë¸ì¼ë ˆë¸': [r'ì„¸ë¸ì¼ë ˆë¸', r'7-eleven', r'ì„¸ë¸'],
            'ì´ë§ˆíŠ¸24': [r'ì´ë§ˆíŠ¸24', r'ì´ë§ˆíŠ¸ì´ì‹­ì‚¬'],
            'ì˜¬ë¦¬ë¸Œì˜': [r'ì˜¬ë¦¬ë¸Œì˜', r'oliveyoung'],
        }
        
        query_lower = query.lower()
        
        for brand_name, patterns in known_brand_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower, re.IGNORECASE):
                    found_brands.append(brand_name)
                    if debug:
                        print(f"   âœ… ë¸Œëœë“œ ë°œê²¬: '{brand_name}' (íŒ¨í„´: {pattern})")
                    break
        
        return list(set(found_brands))

    def _extract_categories_from_query(self, query: str, debug: bool = False) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        found_categories = []
        
        known_category_patterns = {
            'ì¹´í˜': [r'ì¹´í˜', r'ì»¤í”¼', r'coffee', r'cafe', r'ì»¤í”¼ìˆ', r'ì•„ë©”ë¦¬ì¹´ë…¸', r'ë¼ë–¼'],
            'ë§ˆíŠ¸': [r'ë§ˆíŠ¸', r'mart', r'ìŠˆí¼', r'ëŒ€í˜•ë§ˆíŠ¸', r'í• ì¸ë§ˆíŠ¸', r'ì‡¼í•‘ëª°'],
            'í¸ì˜ì ': [r'í¸ì˜ì ', r'í¸ì˜', r'ì»¨ë¹„ë‹ˆ', r'convenience'],
            'ì˜¨ë¼ì¸ì‡¼í•‘': [r'ì˜¨ë¼ì¸', r'ì‡¼í•‘', r'ì¸í„°ë„·', r'online', r'shopping', r'ë°°ì†¡'],
            'ì‹ë‹¹': [r'ì‹ë‹¹', r'ìŒì‹ì ', r'ë ˆìŠ¤í† ë‘', r'restaurant', r'ìŒì‹', r'ì¹˜í‚¨', r'ë²„ê±°'],
            'ë·°í‹°': [r'ë·°í‹°', r'í™”ì¥í’ˆ', r'ë¯¸ìš©', r'beauty', r'cosmetic', r'ìŠ¤í‚¨ì¼€ì–´'],
            'ì˜ë£Œ': [r'ì˜ë£Œ', r'ì•½êµ­', r'ë³‘ì›', r'pharmacy', r'medical', r'ê±´ê°•'],
            'êµí†µ': [r'êµí†µ', r'ì§€í•˜ì² ', r'ë²„ìŠ¤', r'íƒì‹œ', r'ì „ì² ', r'ëŒ€ì¤‘êµí†µ']
        }
        
        query_lower = query.lower()
        
        for category_name, patterns in known_category_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query_lower, re.IGNORECASE):
                    found_categories.append(category_name)
                    if debug:
                        print(f"   âœ… ì¹´í…Œê³ ë¦¬ ë°œê²¬: '{category_name}' (íŒ¨í„´: {pattern})")
                    break
        
        return list(set(found_categories))

    def _is_personalization_query(self, query: str) -> bool:
        """ê°œì¸í™” ìš”ì²­ì¸ì§€ íŒë‹¨"""
        personalization_patterns = [
            r'ë‚´\s*ì†Œë¹„.*íŒ¨í„´', r'ë‚´.*ë§ëŠ”', r'ë‚˜.*ë§ëŠ”', r'ìš°ë¦¬.*ë§ëŠ”',
            r'ê°œì¸í™”.*ì¶”ì²œ', r'ë§ì¶¤.*ì¶”ì²œ', r'ë§ì¶¤í˜•.*í˜œíƒ',
            r'ì§€ë‚œ.*ì†Œë¹„', r'ìµœê·¼.*ì†Œë¹„', r'ì €ë²ˆ.*ì†Œë¹„',
            r'ì§€ë‚œì£¼.*ì¼', r'ì €ë²ˆì£¼.*ì¼', r'ìµœê·¼.*ì¼',
            r'ë‚´ê°€.*ìì£¼', r'ë‚´ê°€.*ë§ì´', r'ë‚´ê°€.*ì£¼ë¡œ',
            r'íŒ¨í„´.*ê¸°ë°˜', r'ì´ë ¥.*ê¸°ë°˜', r'íˆìŠ¤í† ë¦¬.*ê¸°ë°˜'
        ]
        
        for pattern in personalization_patterns:
            if re.search(pattern, query, re.IGNORECASE):
                return True
        
        return False

    def _validate_query_improved(self, query: str, analysis: Dict, parsed_info: Dict, 
                            extracted_brands: List[str], is_personalization: bool, debug: bool) -> Dict[str, Any]:
        """ê°œì„ ëœ ì¿¼ë¦¬ ê²€ì¦"""
        if debug:
            print("ğŸ”§ ê°œì„ ëœ ì¿¼ë¦¬ ê²€ì¦ ì‹œì‘...")
        
        # ê°œì¸í™” ìš”ì²­ì´ë©´ ë¬´ì¡°ê±´ í†µê³¼
        if is_personalization:
            if debug:
                print("   âœ… ê°œì¸í™” ìš”ì²­ìœ¼ë¡œ ì¸ì‹ - ê²€ìƒ‰ ì§„í–‰")
            return {'valid': True}
        
        # ë¸Œëœë“œê°€ ì¶”ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        if extracted_brands:
            brand_existence = self._check_brand_existence(extracted_brands, debug)
            missing_brands = [brand for brand, exists in brand_existence.items() if not exists]
            
            if missing_brands and len(missing_brands) == len(extracted_brands):
                if debug:
                    print(f"   âŒ ì¶”ì¶œëœ ëª¨ë“  ë¸Œëœë“œ '{', '.join(missing_brands)}'ê°€ DBì— ì—†ìŒ")
                return {
                    'valid': False,
                    'message': f"ë¸Œëœë“œ '{', '.join(missing_brands)}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                }
        else:
            # ë¸Œëœë“œê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ì§€ë§Œ íŠ¹ì • ì œí’ˆëª…ì´ í¬í•¨ëœ ê²½ìš° ì²´í¬
            unknown_product_patterns = [r'ì• í”Œì›Œì¹˜', r'ì•„ì´í°', r'ê°¤ëŸ­ì‹œ', r'ë§¥ë¶', r'ì•„ì´íŒ¨ë“œ']
            for pattern in unknown_product_patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    if debug:
                        print(f"   âŒ ì•Œë ¤ì§€ì§€ ì•Šì€ ì œí’ˆ '{pattern}' ê°ì§€")
                    return {
                        'valid': False,
                        'message': f"'{pattern}'ì— ëŒ€í•œ ì •ë³´ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
                    }
        
        # ê¸°ë³¸ì ìœ¼ë¡œ í†µê³¼
        return {'valid': True}

    def _check_brand_existence(self, brands: List[str], debug: bool = False) -> Dict[str, bool]:
        """ë¸Œëœë“œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        if not self.db_metadata_loaded:
            return {brand: True for brand in brands}
        
        result = {}
        for brand in brands:
            exists = brand in self.available_brands or any(
                brand.lower() in available_brand.lower() or 
                available_brand.lower() in brand.lower()
                for available_brand in self.available_brands
            )
            result[brand] = exists
            
            if debug:
                status = "âœ… ì¡´ì¬" if exists else "âŒ ì—†ìŒ"
                print(f"   ğŸ” '{brand}': {status}")
        
        return result

    def _try_direct_brand_search(self, query: str, top_k: int, debug: bool = False) -> List[Dict]:
        """ë¸Œëœë“œ ê¸°ë°˜ ì§ì ‘ ê²€ìƒ‰"""
        try:
            extracted_brands = self._extract_brands_from_query(query, debug)
            
            if not extracted_brands:
                return []
            
            all_results = []
            
            for brand in extracted_brands:
                try:
                    brand_results = self.collection.get(
                        where={"brand": {"$eq": brand}},
                        include=["metadatas", "documents"]
                    )
                    
                    if brand_results and brand_results.get('metadatas'):
                        for i, metadata in enumerate(brand_results['metadatas']):
                            if self._validate_result(metadata, datetime.now()):
                                all_results.append({
                                    "id": f"direct_{brand}_{i}",
                                    "metadata": metadata,
                                    "distance": 0.0,
                                    "document": brand_results['documents'][i] if brand_results.get('documents') else "",
                                    "vector_rank": i + 1
                                })
                except Exception as e:
                    if debug:
                        print(f"   '{brand}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            return all_results[:top_k]
            
        except Exception as e:
            if debug:
                print(f"âŒ ì§ì ‘ ë¸Œëœë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _try_direct_category_search(self, query: str, categories: List[str], top_k: int, debug: bool = False) -> List[Dict]:
        """ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ì§ì ‘ ê²€ìƒ‰"""
        try:
            all_results = []
            
            for category in categories:
                try:
                    category_results = self.collection.get(
                        where={"category": {"$eq": category}},
                        include=["metadatas", "documents"]
                    )
                    
                    if category_results and category_results.get('metadatas'):
                        for i, metadata in enumerate(category_results['metadatas']):
                            if self._validate_result(metadata, datetime.now()):
                                all_results.append({
                                    "id": f"direct_cat_{category}_{i}",
                                    "metadata": metadata,
                                    "distance": 0.0,
                                    "document": category_results['documents'][i] if category_results.get('documents') else "",
                                    "vector_rank": i + 1
                                })
                except Exception as e:
                    if debug:
                        print(f"   '{category}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                    continue
            
            return all_results[:top_k]
            
        except Exception as e:
            if debug:
                print(f"âŒ ì§ì ‘ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _fallback_text_search(self, query: str, filters: Dict, top_k: int, debug: bool = False) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ê¸°ë°˜ í´ë°± ê²€ìƒ‰"""
        try:
            all_results = self.collection.get(include=["metadatas", "documents"])
            
            if not all_results or not all_results.get('metadatas'):
                return []
            
            scored_results = []
            query_lower = query.lower()
            query_words = query_lower.split()
            
            for i, metadata in enumerate(all_results['metadatas']):
                if not metadata or not self._validate_result(metadata, datetime.now()):
                    continue
                
                score = 0.0
                
                # ë¸Œëœë“œ ë§¤ì¹­ (60%)
                brand = metadata.get('brand', '').lower()
                if brand and brand in query_lower:
                    score += 0.6
                elif brand and any(word in brand for word in query_words):
                    score += 0.4
                
                # ì¹´í…Œê³ ë¦¬ ë§¤ì¹­ (20%)
                category = metadata.get('category', '').lower()
                if category and category in query_lower:
                    score += 0.2
                
                # ì œëª© ë§¤ì¹­ (15%)
                title = metadata.get('title', '').lower()
                if title:
                    matching_words = sum(1 for word in query_words if word in title)
                    score += 0.15 * (matching_words / len(query_words) if query_words else 0)
                
                # í˜œíƒ íƒ€ì… ë§¤ì¹­ (5%)
                benefit_type = metadata.get('benefit_type', '').lower()
                benefit_keywords = ['í• ì¸', 'ì ë¦½', 'ì¿ í°', 'í˜œíƒ', 'ì´ë²¤íŠ¸', 'ì¦ì •']
                if any(keyword in query_lower for keyword in benefit_keywords):
                    if benefit_type in query_lower:
                        score += 0.05
                
                if score > 0:
                    scored_results.append({
                        "id": f"text_match_{i}",
                        "metadata": metadata,
                        "distance": 1.0 - score,
                        "document": all_results['documents'][i] if all_results.get('documents') else "",
                        "vector_rank": 0,
                        "text_score": score
                    })
            
            scored_results.sort(key=lambda x: x['text_score'], reverse=True)
            return scored_results[:top_k]
            
        except Exception as e:
            if debug:
                print(f"âŒ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []

    def _apply_personalization_scoring_readonly(self, results: List[Dict], user_profile: Dict, 
                                              parsed_info: Dict, debug: bool) -> List[Dict]:
        """ê°œì¸í™” ìŠ¤ì½”ì–´ë§"""
        if not results:
            return []
        
        scored_results = []
        current_date = datetime.now()
        all_distances = [result.get('distance', 0) for result in results]
        
        for result in results:
            try:
                metadata = result.get('metadata', {})
                
                if not self._validate_result(metadata, current_date):
                    continue
                
                # ê¸°ë³¸ ë²¡í„° ì ìˆ˜
                vector_score = self.calculate_vector_similarity_universal(
                    result.get('distance', 0), all_distances
                )
                
                # ê°œì¸í™” ì ìˆ˜ (ê°„ë‹¨ ë²„ì „)
                personalized_score = vector_score * 0.8 + 0.2
                
                result['personalized_score'] = personalized_score
                result['vector_score'] = vector_score
                
                scored_results.append(result)
                
            except Exception as e:
                if debug:
                    print(f"      âŒ ìŠ¤ì½”ì–´ë§ ì˜¤ë¥˜: {e}")
                continue
        
        return scored_results

    def calculate_vector_similarity_universal(self, distance: float, all_distances: List[float] = None) -> float:
        """ë§ŒëŠ¥ ë²¡í„° ìœ ì‚¬ë„ ê³„ì‚°"""
        # ìŒìˆ˜ ê±°ë¦¬ê°’ ì²˜ë¦¬ (IP ë°©ì‹)
        if distance < 0:
            if all_distances:
                min_dist = min(all_distances)
                max_dist = max(all_distances)
                range_dist = max_dist - min_dist
                
                if range_dist > 0:
                    relative_pos = (distance - min_dist) / range_dist
                    similarity = 1 - relative_pos
                else:
                    similarity = 0.5
            else:
                normalized = max(0, min(1, (distance + 1000) / 200))
                similarity = normalized
        else:
            # ì–‘ìˆ˜ ê±°ë¦¬ê°’ ì²˜ë¦¬
            similarity = max(0, 1 - distance)
        
        return max(0, min(similarity, 1))

    def _validate_result(self, metadata: Dict, current_date: datetime) -> bool:
        """ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦"""
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        if not all([metadata.get('brand'), metadata.get('category'), 
                   metadata.get('title'), metadata.get('benefit_type')]):
            return False
        
        # í™œì„± ìƒíƒœ ê²€ì¦
        if not metadata.get('is_active', False):
            return False
        
        # ë‚ ì§œ ìœ íš¨ì„± ê²€ì¦
        if metadata.get('valid_to'):
            try:
                valid_to = datetime.fromisoformat(metadata['valid_to'])
                if valid_to < current_date:
                    return False
            except:
                return False
        
        return True

    def _rank_and_select_results(self, results: List[Dict], user_profile: Dict, 
                               top_k: int, debug: bool) -> List[Dict]:
        """ìµœì¢… ìˆœìœ„ ê²°ì • ë° ê²°ê³¼ ì„ íƒ"""
        if not results:
            return []
        
        try:
            sorted_results = sorted(results, key=lambda x: x.get('personalized_score', 0), reverse=True)
        except Exception as e:
            return results[:top_k]
        
        return sorted_results[:top_k]

    def _generate_results_readonly(self, results: List[Dict], user_profile: Dict, parsed_info: Dict = None) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ ìƒì„± - JSON í˜•ì‹ìœ¼ë¡œ ì´ë²¤íŠ¸ ê¸°ê°„ í¬í•¨"""
        if not results:
            # ê²°ê³¼ê°€ ì—†ì„ ë•Œ ë¹ˆ JSON ë°°ì—´ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            return json.dumps([], ensure_ascii=False, indent=4)
        
        try:
            # ìµœì¢… ê²°ê³¼ë¥¼ ë‹´ì„ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            output_results = []
            
            for result in results[:5]:
                metadata = result.get('metadata', {})
                score = result.get('personalized_score', 0)
                
                # 1. í˜œíƒ ë‚´ìš© ìš”ì•½ ìƒì„±
                benefit_type = metadata.get('benefit_type', 'Unknown')
                discount_rate = metadata.get('discount_rate', 0)
                conditions = metadata.get('conditions', 'ì¡°ê±´ ì—†ìŒ')
                
                try:
                    discount_rate_float = float(discount_rate) if discount_rate else 0
                except ValueError:
                    discount_rate_float = 0

                benefit_summary = f"{benefit_type}"
                if benefit_type == "í• ì¸" and discount_rate_float > 0:
                    benefit_summary = f"{discount_rate_float}% í• ì¸"
                elif benefit_type == "ì ë¦½" and discount_rate_float > 0:
                    benefit_summary = f"{discount_rate_float}ë°° ì ë¦½"
                
                if conditions != 'ì¡°ê±´ ì—†ìŒ':
                    benefit_summary += f" ({conditions})"

                # 2. ì´ë²¤íŠ¸ ê¸°ê°„ ë¬¸ìì—´ ìƒì„±
                valid_from = metadata.get('valid_from', '')
                valid_to = metadata.get('valid_to', '')
                
                if valid_from and valid_to:
                    event_period = f"{valid_from} ~ {valid_to}"
                elif valid_to:
                    event_period = f"~ {valid_to}ê¹Œì§€"
                elif valid_from:
                    event_period = f"{valid_from}ë¶€í„°"
                else:
                    event_period = "ìƒì‹œ ì§„í–‰"

                # 3. ê³ ê° íƒ€ì… ë¬¸ìì—´ ìƒì„±
                brand = metadata.get('brand', 'Unknown')
                customer_type_map = {
                    "ìŠ¤íƒ€ë²…ìŠ¤": "ì¹´ê³µ/ì—…ë¬´ìš© ì„ í˜¸ ê³ ê°",
                    "ì´ë””ì•¼": "ê°€ì„±ë¹„ ì¤‘ì‹œ ê³ ê°",
                    "ë§¥ë„ë‚ ë“œ": "í¸ì˜ì„± ì¤‘ì‹œ ê³ ê°",
                    "ì´ë§ˆíŠ¸": "ëŒ€ìš©ëŸ‰ êµ¬ë§¤ ì„ í˜¸ ê³ ê°",
                    "GS25": "ì ‘ê·¼ì„± ì¤‘ì‹œ ê³ ê°",
                    "ì˜¬ë¦¬ë¸Œì˜": "ë·°í‹°/ì¼€ì–´ ê´€ì‹¬ ê³ ê°"
                }
                customer_type = customer_type_map.get(brand, "ì¼ë°˜ ê³ ê°")

                # 4. JSON ê°ì²´ë¡œ ë§Œë“¤ ì •ë³´ë¥¼ ë”•ì…”ë„ˆë¦¬ì— ë‹´ìŠµë‹ˆë‹¤.
                item_info = {
                    "brand": brand,
                    "event_name": metadata.get('title', 'Unknown'),
                    "benefit": benefit_summary,
                    "period": event_period,
                    "recommended_for": customer_type,
                    "suitability": f"{score*100:.0f}%"
                }
                
                # ë¦¬ìŠ¤íŠ¸ì— ë”•ì…”ë„ˆë¦¬ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
                output_results.append(item_info)
                
            # ìµœì¢…ì ìœ¼ë¡œ ë¦¬ìŠ¤íŠ¸ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
            # ensure_ascii=FalseëŠ” í•œê¸€ì´ ê¹¨ì§€ì§€ ì•Šê²Œ í•©ë‹ˆë‹¤.
            # indent=4ëŠ” ê°€ë…ì„±ì„ ìœ„í•´ JSONì„ ì˜ˆì˜ê²Œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
            return json.dumps(output_results, ensure_ascii=False, indent=4)
            
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì—ëŸ¬ ì •ë³´ë¥¼ ë‹´ì€ JSONì„ ë°˜í™˜í•©ë‹ˆë‹¤.
            error_message = {
                "error": "ê²°ê³¼ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "details": str(e)
            }
            return json.dumps(error_message, ensure_ascii=False, indent=4)

    # ======================================================================================
    # ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    # ======================================================================================
    
    def run(self, query: str, user_history: List[Dict] = None, debug: bool = False) -> str:
        """RAG ì‹œìŠ¤í…œ ì‹¤í–‰"""
        if user_history is None:
            user_history = create_sample_user_history()
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        initial_state: RAGState = {
            "query": query,
            "user_history": user_history,
            "debug": debug,
            
            # ì´ˆê¸°í™”í•  í•„ë“œë“¤
            "query_analysis": {},
            "parsed_info": {},
            "filters": {},
            "user_profile_data": {},
            "extracted_brands": [],
            "extracted_categories": [],
            "is_personalization": False,
            "validation_result": {},
            "direct_search_results": [],
            "vector_search_results": [],
            "text_search_results": [],
            "perplexity_results": "",
            "personalized_results": [],
            "final_results": [],
            "response": "",
            "next_action": "",
            "error_message": ""
        }
        
        try:
            # ê·¸ë˜í”„ ì‹¤í–‰
            final_state = self.graph.invoke(initial_state)
            return final_state["response"]
            
        except Exception as e:
            return f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}"


# ======================================================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ======================================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ LangGraph ê¸°ë°˜ ê°œì¸í™” í˜œíƒ ì¶”ì²œ RAG ì‹œìŠ¤í…œ")
    print("=" * 80)
    
    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    rag = LangGraphRAGSystem()
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    if not rag.connect_database():
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨. ë”ë¯¸ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "ìŠ¤íƒ€ë²…ìŠ¤ í• ì¸ í˜œíƒ ìˆì–´?",
        "ë‚´ ì†Œë¹„ íŒ¨í„´ì— ë§ëŠ” í˜œíƒ ì¶”ì²œí•´ì¤˜",
        "ì¹´í˜ í• ì¸ ì´ë²¤íŠ¸ ê¶ê¸ˆí•´",
        "ì• í”Œì›Œì¹˜ í• ì¸",  # DBì— ì—†ìŒ -> Perplexity
        "í¸ì˜ì  ì¿ í° ìˆë‚˜?"
    ]
    
    print("\nğŸ§ª ìë™ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[í…ŒìŠ¤íŠ¸ {i}] {query}")
        print("-" * 50)
        
        try:
            result = rag.run(query, debug=False)
            print(f"ğŸ“‹ ê²°ê³¼:\n{result}")
        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
        
        print("-" * 50)
    
    print("\nâœ… ìë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    # ëŒ€í™”í˜• ëª¨ë“œ
    print("\nğŸ’¬ ëŒ€í™”í˜• ëª¨ë“œ ì‹œì‘ (ì¢…ë£Œ: 'quit')")
    
    while True:
        try:
            query = input("\nğŸ”§ ì§ˆë¬¸: ").strip()
            
            if query.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ ì„œë¹„ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not query:
                continue
            
            print("\nâ³ LangGraph RAG ê²€ìƒ‰ ì¤‘...")
            result = rag.run(query, debug=True)
            print(f"\nğŸ”— ì¶”ì²œ ê²°ê³¼:\n{result}")
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì‚¬ìš©ìê°€ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    main()
